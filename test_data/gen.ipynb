{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete: 878 train / 220 val\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Input and output paths\n",
    "input_file = \"dogs2.jsonl\"\n",
    "train_file = \"train.jsonl\"\n",
    "val_file = \"val.jsonl\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Load all records\n",
    "with open(input_file, \"r\") as f:\n",
    "    records = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(records)\n",
    "split_idx = int(0.8 * len(records))\n",
    "train_records = records[:split_idx]\n",
    "val_records = records[split_idx:]\n",
    "\n",
    "# Save train split\n",
    "with open(train_file, \"w\") as f:\n",
    "    for rec in train_records:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "# Save validation split\n",
    "with open(val_file, \"w\") as f:\n",
    "    for rec in val_records:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Split complete: {len(train_records)} train / {len(val_records)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total images checked: 878\n",
      "🔽 Min size: 640x640 = 409600 pixels\n",
      "🔼 Max size: 640x640 = 409600 pixels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def get_image_sizes(jsonl_path):\n",
    "    records = load_jsonl(jsonl_path)\n",
    "    sizes = []\n",
    "\n",
    "    for i, rec in enumerate(records):\n",
    "        try:\n",
    "            image_path = rec.get(\"image\")\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert(\"RGB\")\n",
    "                width, height = img.size\n",
    "                sizes.append((width, height))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping index {i}: {e}\")\n",
    "\n",
    "    if not sizes:\n",
    "        print(\"❌ No valid images found.\")\n",
    "        return\n",
    "\n",
    "    # Convert to total pixels for sorting\n",
    "    sizes_by_area = [(w, h, w * h) for w, h in sizes]\n",
    "    min_size = min(sizes_by_area, key=lambda x: x[2])\n",
    "    max_size = max(sizes_by_area, key=lambda x: x[2])\n",
    "\n",
    "    print(f\"✅ Total images checked: {len(sizes)}\")\n",
    "    print(f\"🔽 Min size: {min_size[0]}x{min_size[1]} = {min_size[2]} pixels\")\n",
    "    print(f\"🔼 Max size: {max_size[0]}x{max_size[1]} = {max_size[2]} pixels\")\n",
    "\n",
    "# Example usage\n",
    "get_image_sizes(\"train.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Wrote 878 records to train.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "jsonl_to_json.py\n",
    "Convert file.jsonl  ➜  file.json\n",
    "Usage:  python jsonl_to_json.py input.jsonl output.json\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def jsonl_to_json(src_path: str, dst_path: str) -> None:\n",
    "    # Load every line as an individual JSON object\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    # Save as one JSON array (pretty-printed, 2-space indent)\n",
    "    with open(dst_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅  Wrote {len(records)} records to {dst_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    src, dst = \"train.jsonl\", \"train.json\"\n",
    "    if not Path(src).is_file():\n",
    "        sys.exit(f\"❌  Input file not found: {src}\")\n",
    "\n",
    "    jsonl_to_json(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Wrote 878 records to train_prefix.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PREFIX = \"../../../..\"          # what you want to prepend\n",
    "\n",
    "\n",
    "def process_json(src: Path, dst: Path) -> None:\n",
    "    \"\"\"Handle a single JSON array.\"\"\"\n",
    "    records = json.load(src.open())\n",
    "    for rec in records:\n",
    "        if \"image\" in rec:\n",
    "            rec[\"image\"] = f\"{PREFIX}{rec['image']}\"\n",
    "    json.dump(records, dst.open(\"w\"), ensure_ascii=False, indent=2)\n",
    "    print(f\"✅  Wrote {len(records)} records to {dst}\")\n",
    "\n",
    "\n",
    "def process_jsonl(src: Path, dst: Path) -> None:\n",
    "    \"\"\"Handle line-delimited JSON (jsonl).\"\"\"\n",
    "    total = 0\n",
    "    with src.open() as fin, dst.open(\"w\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            if \"image\" in rec:\n",
    "                rec[\"image\"] = f\"{PREFIX}{rec['image']}\"\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            total += 1\n",
    "    print(f\"✅  Wrote {total} lines to {dst}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    src_path = Path(\"train.json\")\n",
    "    dst_path = Path(\"train_prefix.json\")\n",
    "\n",
    "\n",
    "    process_json(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote train_wtoken.json\n"
     ]
    }
   ],
   "source": [
    "import json, sys, os\n",
    "TOKEN = \"<image>\\n\"          # or whatever DEFAULT_IMAGE_TOKEN equals\n",
    "\n",
    "src = \"train_prefix.json\"      # input\n",
    "dst = \"train_wtoken.json\"\n",
    "\n",
    "records = json.load(open(src))\n",
    "for rec in records:\n",
    "    if \"image\" in rec:\n",
    "        first = rec[\"conversations\"][0]\n",
    "        if first[\"from\"] == \"human\" and TOKEN not in first[\"value\"]:\n",
    "            first[\"value\"] = f\"{TOKEN}{first['value']}\"\n",
    "\n",
    "json.dump(records, open(dst, \"w\"), ensure_ascii=False, indent=2)\n",
    "print(\"✅ wrote\", dst)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
